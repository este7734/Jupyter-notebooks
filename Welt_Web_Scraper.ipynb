{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welt_Web_Scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HcOlndfh9XwA"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/este7734/Jupyter-notebooks/blob/master/Welt_Web_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bps20joHXZod",
        "colab_type": "text"
      },
      "source": [
        "# <font color='orange'> Welt Web Scraper </font>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWXpudN7R3Qc",
        "colab_type": "text"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_F9Q7Td-sRVi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9120ef70-a16c-41d6-a6ff-0a6e9b6fb007"
      },
      "source": [
        "# Import libraries for processing web text\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from lxml import html\n",
        "\n",
        "# Import these dependencies if using Google Colab \n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLQoyopg1oiY",
        "colab_type": "text"
      },
      "source": [
        "## Define All Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "mh-Z_yZsENF2",
        "colab": {}
      },
      "source": [
        "# Get content of the webpage in an html string format by passing a url \n",
        "def get_html(url):\n",
        "    page = requests.get(url)\n",
        "    html_out = html.fromstring(page.content)\n",
        "    text = page.text\n",
        "    return html_out, text\n",
        "\n",
        "# Convert html into soup to enable soup menthods\n",
        "def get_soup(html_string):\n",
        "    soup = BeautifulSoup(html_string, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "# Extract hyperlinks from soup\n",
        "def get_soup_links(soup):\n",
        "    links = []\n",
        "    for link in soup.find_all('a'):\n",
        "        out_link = link.get('href')\n",
        "        links.append(out_link)\n",
        "    return links\n",
        "\n",
        "# This function is for use with only the Topic pages on reuters.com\n",
        "# Search through ALL links and filter for only those that are for actual articles\n",
        "# links are formatted differently \n",
        "def get_articles_reuters_topics(links, old_url_set):\n",
        "    articles = []\n",
        "    for link in links:\n",
        "        try:\n",
        "            split_link = link.split('/')\n",
        "            for part in split_link:\n",
        "              if part.startswith('article'):\n",
        "                link = 'https://www.welt.de' + link\n",
        "                if url_check(old_url_set, link) == False:\n",
        "                  articles.append(link)\n",
        "        except:\n",
        "            continue\n",
        "    articles = list(set(articles))\n",
        "    old_url_set = set(articles + list(old_url_set))       \n",
        "    return articles, old_url_set\n",
        "\n",
        "# Check if new urls exists in the old_url_set. if yes, return True; if no, return False\n",
        "# This function is used in the get_articles_reuters_topics function\n",
        "def url_check(old_url_set, url):\n",
        "    url_set = set([url])\n",
        "    test_set = old_url_set & url_set\n",
        "    if len(test_set) == 0:\n",
        "        check = False\n",
        "    else:\n",
        "        check = True\n",
        "    return check\n",
        "\n",
        "# Get html strings from list of article weblinks\n",
        "def get_html_reuters(articles):\n",
        "    soup_list = []\n",
        "    for article in articles:\n",
        "        _, text = get_html(article)\n",
        "        soup = get_soup(text)\n",
        "        soup_list.append(soup)\n",
        "    return soup_list"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBmEliCH2G9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Break out article_body, article_headline, and article_date from each article in provided hyperlinks and put into a dictionary called: out_list\n",
        "def get_reuters_elements(soup_list, articles):\n",
        "    out_list = []\n",
        "    i = 0\n",
        "    for article in soup_list:\n",
        "        link = articles[i] # I don't think this is used at all here, which means there is no reason to require the second argument: articles\n",
        "        i += 1\n",
        "        try:\n",
        "            article_body = article.find_all(body_class, {'class': body_tag})\n",
        "            article_p = []\n",
        "            for item in article_body:\n",
        "                p_list = item.find_all('p')\n",
        "                for p in p_list:\n",
        "                    article_p.append(p.text)\n",
        "            out_text = ' '.join(article_p)\n",
        "            if out_text == '':\n",
        "              continue\n",
        "            if out_text.startswith('Besondere Reportagen'):\n",
        "              continue\n",
        "            out_dict = dict([('Text',out_text),('url',link)])\n",
        "            out_list.append(out_dict)\n",
        "        except:\n",
        "            print('Unable to decode...skipping article...')\n",
        "            continue\n",
        "\n",
        "    return out_list"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wVxrANXEF-W",
        "colab_type": "text"
      },
      "source": [
        "Tags for different websites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBqKx4fM92CS",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Welt classes and tags\n",
        "body_class = 'div'\n",
        "headline_class = 'h2'\n",
        "date_class = 'div'\n",
        "\n",
        "body_tag = 'c-sticky-container'\n",
        "headline_tag = 'c-headline o-dreifaltigkeit__headline rf-o-headline'\n",
        "date_tag = 'c-container c-container--is-stacked'"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_qCIHy31uSJ",
        "colab_type": "text"
      },
      "source": [
        "## Define URL Variables and Run Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU0yQpNqLwl4",
        "colab_type": "text"
      },
      "source": [
        "Step 1. Instantiate `old_url_set` to be used in the `get_articles_reuters_topics` function. This is a running log of article links that will be compiled by iterating from steps 2 - 6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYZGQ8Aq93Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_articles_reuters_topics(links, old_url_set):\n",
        "    articles = []\n",
        "    for link in links:\n",
        "        try:\n",
        "            if 'https://www.welt.de' not in link:\n",
        "                if 'article' in link:\n",
        "                  link = 'https://www.welt.de' + link\n",
        "                  if url_check(old_url_set, link) == False:\n",
        "                    articles.append(link)\n",
        "        except:\n",
        "            continue\n",
        "    articles = list(set(articles))\n",
        "    old_url_set = set(articles + list(old_url_set))       \n",
        "    return articles, old_url_set"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEA09SnTJhj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S4mzVpzp5g1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate empty set to use a running list of hyperlinks while \n",
        "# running the scrape iterations\n",
        "old_url_set = set([])"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBkhWZ5iMWq9",
        "colab_type": "text"
      },
      "source": [
        "## Scrape Reuters Topics pages for all the most recent news articles. <font color='orange'>*Run Steps 2 - 7 for each instance of `url` variable, before moving on to the next steps*</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URLqBDPwUbRZ",
        "colab_type": "text"
      },
      "source": [
        "<font color='orange'>Step 2.</font> Define variables for each of Reuters main topics pages. Run this cell for each iteration by uncommenting a different url each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8a1cAaw3PfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define url variables\n",
        "# NOTE: You must run these individually through the end of this section\n",
        "# I didn't have time to figure out how to loop through all of them properly\n",
        "# There is a section at the very bottom where you can see that I attempted but ran\n",
        "# into a problem on one of the last functions. \n",
        "\n",
        "# Welt Links\n",
        "#url = r'https://www.welt.de/'\n",
        "url = r'https://www.welt.de/politik/'\n",
        "#url = r'https://www.welt.de/wirtschaft/'\n",
        "#url = r'https://www.welt.de/vermischtes/'\n",
        "#url = r'https://www.welt.de/debatte/'"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qGyTkHR3Tos",
        "colab_type": "text"
      },
      "source": [
        "## Scraper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctx4uUaNM-qy",
        "colab_type": "text"
      },
      "source": [
        "<font color='orange'>Step 3.</font> Get HTML srting from web `url`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJshIC2L4iPA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9a1e73c1-24f8-465f-982b-0e239e00fef0"
      },
      "source": [
        "# Pass the each instance of `url` variable to return the web page in HTML format and convert it to a string\n",
        "html_string = str(get_html(url))\n",
        "# Pass the HTML string (of the web page) to get its soup\n",
        "soup = get_soup(html_string)\n",
        "# Find ALL links on within the soup\n",
        "links = get_soup_links(soup)\n",
        "# Use this for Topics Pages only\n",
        "# Filter out only those links that are for actual articles. We only want the \"good\" links\n",
        "# This filters out things like links to images and advertisements or non-news worthy pages\n",
        "articles, old_url_set = get_articles_reuters_topics(links, old_url_set)\n",
        "print(len(articles))\n",
        "# Print out the running list of hyperlinks to see how many you have\n",
        "print(len(old_url_set))"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37\n",
            "37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJoThpoH39If",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articles\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWBBUihX8R0w",
        "colab_type": "text"
      },
      "source": [
        "## <font color='skyblue'>Parse soup from entire list of hyperlinks that you just accumulated</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5M5Po9_OWk2",
        "colab_type": "text"
      },
      "source": [
        "Step 8. Get soup for every link\n",
        "\n",
        "Step 9. Parse the soup for each link into `article_body`, `article_title`, and `article_date`. Create list of dictionaries for each web page\n",
        "\n",
        "Step 10. Run `TextBlob` on list of dictionaries to separate all sentences in a single list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZoenVkMIukU",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f89d195-4399-4a94-a7e8-dd438795ba2f"
      },
      "source": [
        "# Get soup for each one of the \"good\" links\n",
        "url_links = list(old_url_set) # Convert the running set of links to a list for use in the following functions\n",
        "\n",
        "\n",
        "len(url_links)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsW1_r8Z2Xbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dae0ded6-ac12-4025-eef8-58bf0e932aab"
      },
      "source": [
        "soup_list = get_html_reuters(url_links)\n",
        "print(f'Length of Soup List: {len(soup_list)}')\n",
        "#print(soup_list[0])"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Soup List: 37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_wPuKul2Olq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f271feb-7e74-4e8a-b1b8-6594dafe96c8"
      },
      "source": [
        "# Parse the soup for each \"good\" link to get article text, title, and date\n",
        "out_list = get_reuters_elements(soup_list, url_links)\n",
        "print(f'Length of out_list: {len(out_list)}')\n",
        "#out_list[0:2]"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of out_list: 36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO7ZWsj3GFWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "13bd6b31-53e2-4389-b8d2-91cbd88c5f81"
      },
      "source": [
        "blob_sentences =[]\n",
        "for i in range(len(out_list)):\n",
        "  try:\n",
        "    blob = out_list[i]['Text']\n",
        "    trans = TextBlob(blob) # Enter string object\n",
        "    trans = trans.translate(to='en')\n",
        "    trans = str(trans) # Change to = 'en' for English\n",
        "    trans = TextBlob(trans)\n",
        "    for item in trans.sentences:\n",
        "      blob_sentences.append(trans)\n",
        "    #print('\\n', i+1, '\\n',trans)\n",
        "  except:\n",
        "    print('got an error ...., skipping article....')\n",
        "print(f'\\nYou have {len(blob_sentences)} total sentences from {i+1} different articles')"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "You have 2016 total sentences from 36 different articles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XjoXSZDPHLr",
        "colab_type": "text"
      },
      "source": [
        "This is just a troubleshooting section to ensure you're `TextBlob` came out right. You should see a list of stentences, each starting with the word `Sentence`. Check this before moving to the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4fJQzcl5X3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "395be839-8355-4a8f-8593-a33a0387eb3c"
      },
      "source": [
        "# print the first 6 sentences so you see what it looks like\n",
        "blob_sentences[0]"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"North Rhine-Westphalia's Prime Minister Armin Laschet sees responsibility after the big corona outbreak at meat producer Tönnies. Now the liability of the company should be checked. Source: WELT After the big corona outbreak at meat producer Tönnies, North Rhine-Westphalia's Prime Minister Armin Laschet (CDU) has the company's liability checked. When asked whether the company should be held liable, Laschet told the editorial network Germany (Friday): \"It is currently being examined very carefully whether and which rules the company has violated and where it can be held liable.\" Laschet emphasized that he sees Tönnies as responsible. The prime minister defended his government against criticism of being late in working against the meat company. \"The working conditions in the slaughterhouses were known,\" Laschet told the RND. “Red-green has introduced the work contracts that have become a problem. Our health minister Karl-Josef Laumann was the only one who really pressed for a change. “However, there was no majority for a legal change, said Laschet. \"You have to admit it soberly.\" In a slaughterhouse of the Tönnies company in North Rhine-Westphalia, more than 1500 people have been tested positive for the virus in the past few days. In the Gütersloh district, which was badly hit by the outbreak, numerous people have already had themselves tested voluntarily. The results brought a glimmer of hope, since the tests have shown that there are only a few infected people in the population. The state of North Rhine-Westphalia has temporarily restricted everyday life for the two affected districts Gütersloh and Warendorf until June 30th. Green Party leader Anton Hofreiter asked the entrepreneur Clemens Tönnies on the news portal Watson (Friday) to pay for the cost of the Corona outbreak himself: \"If Mr. Tönnies is serious about his apology to the people in his region, I expect that he pays the costs of the Corona outbreak at Tönnies out of his huge private fortune. And it does not impose a burden on the company and thereby continue to squeeze out its employees. ”The German Patient Protection Foundation, however, accuses the Federal Government of not sufficiently taking care of the infection process in German care facilities. “After just a few days, there were figures on the corona infection process in the meat industry. But how it looks with the more than 800,000 people in need of care in the homes is still unclear, ”said Eugen Brysch, board member of the RND. In Gütersloh, people not only fight for the corona virus, but also for their vacation. If you come from a lockdown area, you need a negative corona test in many places. Source: WELT Federal Minister of Health Jens Spahn (CDU) sees the \"corona crisis\" as a \"completely new feeling of community\" in Germany, as he said in a live talk for the news portals of the Funke media group. He also promoted \"that we should keep this: a kind of corona patriotism\". Spahn warned against stigmatizing people from the affected districts. \"In the end, they tend to be victims of the circumstances,\" said the minister.\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGJmYoaJYF_n",
        "colab_type": "text"
      },
      "source": [
        "## <font color='skyblue'> Keyword Search </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkcNzckEPdOf",
        "colab_type": "text"
      },
      "source": [
        "Step 11. Determine key words used to search for event of interest in all the articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xN0ZSBJ510j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dbeae8a9-048f-4c86-bcff-f92dfbbc7be7"
      },
      "source": [
        "# Allow user to type in key words to search the text for\n",
        "# Note this is case sensitive... so you need to make sure you enter your search \n",
        "# Try the entering the following to see some results: corona,COVID,death\n",
        "filter_list = input(\"Enter key words to search for separated by commas. don't use spaces.\\nSearch is case sensitive.\\nExample search: coronavirus,COVID,death\\n\\n\") #.title() # This is still a string... not a list yet"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter key words to search for separated by commas. don't use spaces.\n",
            "Search is case sensitive.\n",
            "Example search: coronavirus,COVID,death\n",
            "\n",
            "America\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8asVgYSPsbu",
        "colab_type": "text"
      },
      "source": [
        "Step 12. Convert keywords into an iterable list for use in the next step\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49R1H35B7aH8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d789edb7-c978-4ba7-a01c-e0aa57b31fa3"
      },
      "source": [
        "# Split filter words and convert into a list for itterating in the next step \n",
        "\n",
        "f = []\n",
        "for word in (filter_list.split(\",\")):  # Split string into separate words, separate by comma\n",
        "  f.append(word)                       # Generate new list containing each key word\n",
        "f # This is now a list of key words that the user typed in"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['America']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTNasDzQLinR",
        "colab_type": "text"
      },
      "source": [
        "Step 13. <font color='skyblue'> Search </font>  All sentences for Key Words and return only those consisting of a key word\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXjYMd1-LUrT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "7ff31ab1-5630-461b-e925-a9e1ee407342"
      },
      "source": [
        "# Instantiate list for holding the sentences\n",
        "sentences = []\n",
        "\n",
        "# Generate empty list of lists to store the sentences with your key words in them\n",
        "for i in range(len(f)):\n",
        "  sentences.append([])\n",
        "#print('Here is what you just made, and empty list of lists: ', sentences, '\\n')\n",
        "\n",
        "# Generate lists of sentences for each key word and plug them into the list of lists from above            \n",
        "for i in range(len(f)):\n",
        "  for sentence in blob_sentences:\n",
        "    if f[i] in sentence:\n",
        "        sentences[i].append(sentence)\n",
        "        \n",
        "# Print number of sentences containing each key word\n",
        "# Print out all sentences containing each key word\n",
        "for i in range(len(f)):\n",
        "  print('='*200)   \n",
        "  print('\\nThere are {} sentences containing the word: {} '.format(len(sentences[i]), f[i])) \n",
        "  print('-'*200)   \n",
        "  #for sentence in sentences[i]:\n",
        "      #print(sentence)\n",
        "\n",
        "file_Der_Speigel = open(\"MyFile_Welt.txt\", \"w\") \n",
        "file_Der_Speigel.write(str(sentences))\n",
        "file_Der_Speigel.close()      "
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================================================================================================================================================\n",
            "\n",
            "There are 97 sentences containing the word: America \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcOlndfh9XwA",
        "colab_type": "text"
      },
      "source": [
        "## <font color='black'>Step 13. Store All article data in DataFrame </font>  \n",
        "<font color='grey'>Note: This is not just the filtered data, this contains all information from each web page. While not used in this project, it represents all of the original data used in this run. It can be used for reference.</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjj8Wi-4yOSf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "6961d849-3032-4493-9491-f4c35bd282eb"
      },
      "source": [
        "# Put parsed data into Pandas DataFrame\n",
        "pd.DataFrame(out_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In den USA steigen die Zahlen der Neuinfektion...</td>\n",
              "      <td>https://www.spiegel.de/panorama/corona-pandemi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In Mexiko ist nach einem Erdbeben der Stärke 7...</td>\n",
              "      <td>https://www.spiegel.de/panorama/mexiko-erdbebe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kurz vor der Entscheidung der Lufthansa-Aktion...</td>\n",
              "      <td>https://www.spiegel.de/wirtschaft/unternehmen/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Die Corona-Zahlen sind in Deutschland seit der...</td>\n",
              "      <td>https://www.spiegel.de/wissenschaft/medizin/rk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nach den Krawallen in Stuttgart zeigen sich di...</td>\n",
              "      <td>https://www.spiegel.de/panorama/stuttgart-kraw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>Auch der Eiscremehersteller Ben &amp; Jerry's will...</td>\n",
              "      <td>https://www.spiegel.de/netzwelt/web/facebook-w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>Bis zum Ausbruch der Corona-Pandemie galt Wirt...</td>\n",
              "      <td>https://www.spiegel.de/wirtschaft/unternehmen/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>Der frühere Boxweltmeister Roberto Durán ist i...</td>\n",
              "      <td>https://www.spiegel.de/panorama/leute/roberto-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>Ron Jeremy, US-amerikanischer Pornodarsteller,...</td>\n",
              "      <td>https://www.spiegel.de/panorama/leute/ron-jere...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>SPIEGEL: Herr Antinori, Sie haben mehr als ein...</td>\n",
              "      <td>https://www.spiegel.de/wirtschaft/unternehmen/...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>346 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Text                                                url\n",
              "0    In den USA steigen die Zahlen der Neuinfektion...  https://www.spiegel.de/panorama/corona-pandemi...\n",
              "1    In Mexiko ist nach einem Erdbeben der Stärke 7...  https://www.spiegel.de/panorama/mexiko-erdbebe...\n",
              "2    Kurz vor der Entscheidung der Lufthansa-Aktion...  https://www.spiegel.de/wirtschaft/unternehmen/...\n",
              "3    Die Corona-Zahlen sind in Deutschland seit der...  https://www.spiegel.de/wissenschaft/medizin/rk...\n",
              "4    Nach den Krawallen in Stuttgart zeigen sich di...  https://www.spiegel.de/panorama/stuttgart-kraw...\n",
              "..                                                 ...                                                ...\n",
              "341  Auch der Eiscremehersteller Ben & Jerry's will...  https://www.spiegel.de/netzwelt/web/facebook-w...\n",
              "342  Bis zum Ausbruch der Corona-Pandemie galt Wirt...  https://www.spiegel.de/wirtschaft/unternehmen/...\n",
              "343  Der frühere Boxweltmeister Roberto Durán ist i...  https://www.spiegel.de/panorama/leute/roberto-...\n",
              "344  Ron Jeremy, US-amerikanischer Pornodarsteller,...  https://www.spiegel.de/panorama/leute/ron-jere...\n",
              "345  SPIEGEL: Herr Antinori, Sie haben mehr als ein...  https://www.spiegel.de/wirtschaft/unternehmen/...\n",
              "\n",
              "[346 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}